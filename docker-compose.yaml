# docker-compose.yml
version: "3.12"
name: local   # ensures the default network is "local_default" (matches the router's default)

services:
  # --- MLflow tracking server + local artifact store ---
  mlflow:
    image: ghcr.io/mlflow/mlflow:v3.3.2
    command: >
      mlflow server
      --host 0.0.0.0 --port ${MLFLOW_SERVICE_PORT}
      --backend-store-uri sqlite:////mlflowdb/mlflow.db
      --serve-artifacts
      --artifacts-destination file:///mlartifacts
    ports:
      - "${LOCAL_PORT}:${MLFLOW_SERVICE_PORT}"
    volumes:
      - mlflowdb:/mlflowdb
      - mlartifacts:/mlartifacts

  # --- Caddy reverse proxy (fixed public port) ---
  proxy:
    image: caddy:2
    container_name: proxy
    # 9000 is the single public port your users will call
    ports:
      - "9000:9000"
      # (optional) expose admin API to host for debugging:
      # - "2019:2019"
    environment:
      - CADDY_ADMIN=0.0.0.0:2019   # enable Admin API (router uses http://proxy:2019)
    volumes:
      - ./caddy.json:/etc/caddy/caddy.json:ro
    depends_on:
      - mlflow

  # --- Router (control plane: train + roll + proxy flip) ---
  
  router:
    build:
      context: ./router
    environment:
      - TRAINER_SPECS_PATH=/app/trainer-specs
      - TRAINER_SKLEARN_IMAGE=trainer-sklearn-model-1:${TAG:-latest}
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - DOCKER_HOST=http://socket-proxy:2375
    volumes:
      - router_state:/state
    ports:
      - "8010:8000"    # router API
    depends_on:
      - mlflow
      - socket-proxy
      - proxy


  # --- Docker socket proxy (restrict routerâ€™s Docker access) ---
  socket-proxy:
    image: tecnativa/docker-socket-proxy:latest
    privileged: true
    environment:
      - CONTAINERS=1
      - NETWORKS=1
      - POST=1
      - INFO=1
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock

volumes:
  mlflowdb:
  mlartifacts:
  router_state:
